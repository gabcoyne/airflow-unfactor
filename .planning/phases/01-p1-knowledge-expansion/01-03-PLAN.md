---
phase: 01-p1-knowledge-expansion
plan: 03
type: execute
wave: 2
depends_on: [01, 02]
files_modified:
  - tests/test_knowledge.py
autonomous: true
requirements: [KNOW-01, KNOW-02, KNOW-03, KNOW-04, KNOW-05, KNOW-06]

must_haves:
  truths:
    - "colin run compiles all five new model files to JSON without errors"
    - "lookup_concept('KubernetesPodOperator') returns status: found with source: colin"
    - "lookup_concept('DatabricksSubmitRunOperator') returns status: found with source: colin"
    - "lookup_concept('DatabricksRunNowOperator') returns status: found with source: colin"
    - "lookup_concept('SparkSubmitOperator') returns status: found with source: colin"
    - "lookup_concept('SimpleHttpOperator') returns status: found with source: colin"
    - "lookup_concept('SSHOperator') returns status: found with source: colin"
  artifacts:
    - path: "colin/output/operators-kubernetes.json"
      provides: "Compiled KubernetesPodOperator knowledge"
      contains: "KubernetesPodOperator"
    - path: "colin/output/operators-databricks.json"
      provides: "Compiled Databricks operator knowledge"
      contains: "DatabricksSubmitRunOperator"
    - path: "colin/output/operators-spark.json"
      provides: "Compiled SparkSubmitOperator knowledge"
      contains: "SparkSubmitOperator"
    - path: "colin/output/operators-http.json"
      provides: "Compiled HTTP operator knowledge"
      contains: "SimpleHttpOperator"
    - path: "colin/output/operators-sftp.json"
      provides: "Compiled SSH operator knowledge"
      contains: "SSHOperator"
    - path: "tests/test_knowledge.py"
      provides: "Parametrized tests for all six new operators"
      contains: "KubernetesPodOperator"
  key_links:
    - from: "colin/models/operators/*.md"
      to: "colin/output/operators-*.json"
      via: "colin run compilation"
      pattern: "operators-.*\\.json"
    - from: "colin/output/operators-*.json"
      to: "src/airflow_unfactor/knowledge.py"
      via: "load_knowledge reads JSON files"
      pattern: "load_knowledge"
    - from: "tests/test_knowledge.py"
      to: "src/airflow_unfactor/knowledge.py"
      via: "lookup function returns status: found"
      pattern: "lookup.*status.*found"
---

<objective>
Compile the five new Colin model files to JSON using `colin run`, then add parametrized tests to verify that all six operators (KNOW-01 through KNOW-06) are found via `lookup_concept` with `source: colin`.

Purpose: Compilation is the critical integration step — authored Markdown must produce valid JSON. Tests lock in the guarantee that every operator name resolves to a Colin-sourced entry, preventing regression.

Output: Five new JSON files in `colin/output/`, updated test file with parametrized coverage for all six operators, full test suite green.
</objective>

<execution_context>
@/Users/gcoyne/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gcoyne/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-p1-knowledge-expansion/01-RESEARCH.md
@tests/test_knowledge.py
@src/airflow_unfactor/knowledge.py

<interfaces>
<!-- From existing test_knowledge.py -->
```python
from airflow_unfactor.knowledge import load_knowledge, lookup, suggestions

# lookup returns dict with keys: status, source, concept_type, ...
# status: "found" | "not_found"
# source: "colin" | "fallback"
```

<!-- From knowledge.py — load_knowledge reads JSON from a directory -->
<!-- lookup(query, knowledge_dict) → result dict -->
<!-- Colin output JSON files are loaded by load_knowledge(colin_output_dir) -->
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Compile Colin models and verify JSON output</name>
  <files>colin/output/operators-kubernetes.json, colin/output/operators-databricks.json, colin/output/operators-spark.json, colin/output/operators-http.json, colin/output/operators-sftp.json</files>
  <action>
Run `colin run` from the `colin/` directory to compile all model files (existing + new) to JSON.

```bash
cd colin && colin run
```

After compilation, verify that five new JSON files exist in `colin/output/`:
- `operators-kubernetes.json` — must contain "KubernetesPodOperator" key
- `operators-databricks.json` — must contain "DatabricksSubmitRunOperator" and "DatabricksRunNowOperator" keys
- `operators-spark.json` — must contain "SparkSubmitOperator" key
- `operators-http.json` — must contain "SimpleHttpOperator" and "HttpOperator" keys
- `operators-sftp.json` — must contain "SSHOperator" key

If `colin run` fails, check:
1. YAML frontmatter syntax in each new .md file
2. `{% section %}` / `{% endsection %}` tag matching
3. Any file referenced in `_index.md` that doesn't exist

If the output filenames differ from expected (Colin may use a different naming convention), note the actual filenames — the test task will need to reference them correctly.

**Important:** If `colin` CLI is not available in the current environment, the JSON files need to be created manually by converting the Markdown model files to the JSON schema format used by existing output files (see operators-core.json for reference). Create each JSON file as a flat dict mapping operator name → entry fields.
  </action>
  <verify>
    <automated>ls colin/output/operators-*.json | wc -l && python3 -c "import json; [json.load(open(f'colin/output/{f}')) for f in ['operators-kubernetes.json','operators-databricks.json','operators-spark.json','operators-http.json','operators-sftp.json']]; print('All JSON valid')" 2>&1</automated>
  </verify>
  <done>Five new JSON files in colin/output/, each parseable as valid JSON, containing the expected operator keys</done>
</task>

<task type="auto">
  <name>Task 2: Add parametrized tests for all six new operators</name>
  <files>tests/test_knowledge.py</files>
  <action>
Add a new test class `TestPhase1Operators` to `tests/test_knowledge.py` with a parametrized test that loads the actual Colin output and verifies each of the six KNOW-* operators is found.

Add this test class after the existing `TestSuggestions` class:

```python
class TestPhase1Operators:
    """Tests for Phase 1 knowledge expansion operators (KNOW-01 through KNOW-06)."""

    COLIN_OUTPUT_DIR = str(Path(__file__).parent.parent / "colin" / "output")

    @pytest.mark.parametrize("operator_name", [
        "KubernetesPodOperator",      # KNOW-01
        "DatabricksSubmitRunOperator", # KNOW-02
        "DatabricksRunNowOperator",    # KNOW-03
        "SparkSubmitOperator",         # KNOW-04
        "SimpleHttpOperator",          # KNOW-05
        "SSHOperator",                 # KNOW-06
    ])
    def test_operator_found_in_colin_output(self, operator_name):
        """Each Phase 1 operator is found via lookup with source: colin."""
        knowledge = load_knowledge(self.COLIN_OUTPUT_DIR)
        result = lookup(operator_name, knowledge)
        assert result["status"] == "found", f"{operator_name} not found in Colin output"
        assert result["source"] == "colin", f"{operator_name} found but source is {result['source']}, expected colin"
```

Add `from pathlib import Path` and `import pytest` to the imports at the top of the file if not already present.

Then run the full test suite:
```bash
uv run pytest
```

All tests must pass — both existing and new.
  </action>
  <verify>
    <automated>uv run pytest tests/test_knowledge.py -v -x 2>&1 | tail -20</automated>
  </verify>
  <done>All six parametrized test cases pass with status: found and source: colin. Full test suite green.</done>
</task>

</tasks>

<verification>
End-to-end verification of Phase 1 success criteria from ROADMAP.md:
```bash
uv run python -c "
from airflow_unfactor.knowledge import load_knowledge, lookup
k = load_knowledge('colin/output')
for op in ['KubernetesPodOperator', 'DatabricksSubmitRunOperator', 'DatabricksRunNowOperator', 'SparkSubmitOperator', 'SimpleHttpOperator', 'SSHOperator']:
    r = lookup(op, k)
    print(f'{op}: status={r[\"status\"]}, source={r.get(\"source\", \"N/A\")}')
"
```

Expected: All six print `status=found, source=colin`
</verification>

<success_criteria>
1. `colin run` completes without errors (or JSON manually created if CLI unavailable)
2. Five new JSON files exist in `colin/output/`
3. `test_operator_found_in_colin_output` passes for all six operator names
4. Full test suite (`uv run pytest`) passes with 0 failures
5. End-to-end `lookup` calls return `status: found, source: colin` for all six operators
</success_criteria>

<output>
After completion, create `.planning/phases/01-p1-knowledge-expansion/01-03-SUMMARY.md`
</output>
