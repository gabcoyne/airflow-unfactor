---
phase: 01-p1-knowledge-expansion
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - colin/models/operators/databricks.md
  - colin/models/operators/spark.md
  - colin/models/operators/_index.md
autonomous: true
requirements: [KNOW-02, KNOW-03, KNOW-04]

must_haves:
  truths:
    - "databricks.md contains DatabricksSubmitRunOperator section with prefect-databricks DatabricksSubmitRun task mapping"
    - "databricks.md contains DatabricksRunNowOperator section with prefect-databricks DatabricksJobRunNow task mapping"
    - "spark.md contains SparkSubmitOperator section with ShellOperation and Databricks alternative paths"
    - "_index.md references all five new operator model files"
  artifacts:
    - path: "colin/models/operators/databricks.md"
      provides: "DatabricksSubmitRunOperator + DatabricksRunNowOperator translation knowledge"
      contains: "DatabricksSubmitRunOperator"
    - path: "colin/models/operators/spark.md"
      provides: "SparkSubmitOperator translation knowledge with multiple execution paths"
      contains: "SparkSubmitOperator"
    - path: "colin/models/operators/_index.md"
      provides: "Aggregator referencing all operator model files including new ones"
      contains: "operators/kubernetes"
  key_links:
    - from: "colin/models/operators/databricks.md"
      to: "colin run compilation"
      via: "{% section %} format parsed by Colin"
      pattern: "{% section DatabricksSubmitRunOperator %}"
    - from: "colin/models/operators/spark.md"
      to: "colin run compilation"
      via: "{% section %} format parsed by Colin"
      pattern: "{% section SparkSubmitOperator %}"
    - from: "colin/models/operators/_index.md"
      to: "all operator model files"
      via: "{{ ref() }} template includes"
      pattern: "ref\\('operators/"
---

<objective>
Author two Colin model files for Databricks and Spark operator families, and update the _index.md aggregator to reference all five new model files.

Purpose: Databricks operators are the second-most common enterprise integration after Kubernetes, and SparkSubmitOperator requires multi-path guidance (shell, Databricks, Dataproc). The _index.md update ensures `colin run` includes the new files in compilation.

Output: Two new `.md` files in `colin/models/operators/` plus an updated `_index.md` with references to all new operator files.
</objective>

<execution_context>
@/Users/gcoyne/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gcoyne/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-p1-knowledge-expansion/01-RESEARCH.md
@colin/models/operators/aws.md
@colin/models/operators/core.md
@colin/models/operators/_index.md

<interfaces>
<!-- Colin model file format — same as Plan 01 -->

YAML frontmatter:
```yaml
---
name: {Provider Name} Operator Mappings
colin:
  output:
    format: json
---
```

Section structure (every field required, in order):
```
{% section OperatorName %}
## operator
## module
## source_context
## prefect_pattern
## prefect_package
## prefect_import
## example
### before
### after
## notes
{% endsection %}
```

_index.md aggregator pattern (from existing file):
```markdown
## {Provider} Operators

{{ ref('operators/{filename}').content }}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Author databricks.md — DatabricksSubmitRunOperator and DatabricksRunNowOperator Colin models</name>
  <files>colin/models/operators/databricks.md</files>
  <action>
Create `colin/models/operators/databricks.md` with YAML frontmatter (`name: Databricks Provider Operator Mappings`) and TWO sections.

**Section 1: `{% section DatabricksSubmitRunOperator %}`**

- `operator`: DatabricksSubmitRunOperator
- `module`: airflow.providers.databricks.operators.databricks
- `source_context`: Submits a new Databricks job run via the Runs Submit API. Takes a JSON payload defining cluster config, notebook/jar/python task, and libraries. Uses databricks_conn_id for workspace auth. Polls for completion.
- `prefect_pattern`: prefect-databricks jobs_runs_submit task
- `prefect_package`: prefect-databricks
- `prefect_import`: from prefect_databricks import DatabricksCredentials; from prefect_databricks.jobs import jobs_runs_submit
- `example.before`: DatabricksSubmitRunOperator with databricks_conn_id, json payload containing new_cluster and notebook_task (use RESEARCH.md example)
- `example.after`: @task using DatabricksCredentials.load and jobs_runs_submit with same JSON payload (use RESEARCH.md example)
- `notes`:
  - databricks_conn_id → create a DatabricksCredentials block, load with DatabricksCredentials.load("block-name")
  - The JSON payload structure is identical between Airflow and Prefect — pass it through directly
  - prefect-databricks handles polling for run completion automatically
  - Do NOT use raw requests calls to the Databricks REST API; the package handles auth, polling, and error translation
  - For notebook params with Jinja templates like {{ ds }}: replace with explicit Python parameters
- `related_concepts`: ["connection-to-block", "databricks-credentials"]

**Section 2: `{% section DatabricksRunNowOperator %}`**

- `operator`: DatabricksRunNowOperator
- `module`: airflow.providers.databricks.operators.databricks
- `source_context`: Triggers an existing Databricks job by job_id via the Run Now API. Takes job_id and optional notebook_params, jar_params, or python_params. Uses databricks_conn_id for auth. Polls for completion.
- `prefect_pattern`: prefect-databricks jobs_run_now task
- `prefect_package`: prefect-databricks
- `prefect_import`: from prefect_databricks import DatabricksCredentials; from prefect_databricks.jobs import jobs_run_now
- `example.before`: DatabricksRunNowOperator with databricks_conn_id, job_id, notebook_params (use RESEARCH.md example)
- `example.after`: @task using DatabricksCredentials.load and jobs_run_now with job_id and notebook_params (use RESEARCH.md example)
- `notes`:
  - Same credential migration as DatabricksSubmitRunOperator
  - job_id is a direct parameter mapping — no transformation needed
  - notebook_params with {{ ds }} → replace with explicit date parameter passed to the @task
  - Do NOT hardcode conn_id strings; always load credentials from a block
- `related_concepts`: ["connection-to-block", "databricks-credentials"]
  </action>
  <verify>
    <automated>grep -c "{% section Databricks" colin/models/operators/databricks.md && grep -c "{% endsection %}" colin/models/operators/databricks.md && grep -q "DatabricksCredentials" colin/models/operators/databricks.md && echo "PASS" || echo "FAIL"</automated>
  </verify>
  <done>databricks.md exists with DatabricksSubmitRunOperator and DatabricksRunNowOperator sections, both with DatabricksCredentials block migration and explicit anti-patterns</done>
</task>

<task type="auto">
  <name>Task 2: Author spark.md — SparkSubmitOperator Colin model</name>
  <files>colin/models/operators/spark.md</files>
  <action>
Create `colin/models/operators/spark.md` with YAML frontmatter (`name: Spark Provider Operator Mappings`) and one `{% section SparkSubmitOperator %}` block.

This operator has MULTIPLE EXECUTION PATHS. The entry must present all paths with decision guidance.

- `operator`: SparkSubmitOperator
- `module`: airflow.providers.apache.spark.operators.spark_submit
- `source_context`: Submits a Spark application using spark-submit CLI. Takes application path, conf dict, deploy_mode, executor_memory, driver_memory, and other spark-submit flags. Uses conn_id for Spark cluster connection.
- `prefect_pattern`: Option 1 (self-managed Spark): ShellOperation with spark-submit CLI. Option 2 (managed Spark via Databricks): DatabricksSubmitRun with spark_python_task. Option 3 (managed Spark via GCP): Dataproc job submission.
- `prefect_package`: prefect-shell (Option 1), prefect-databricks (Option 2), prefect-gcp (Option 3)
- `prefect_import`: from prefect_shell import ShellOperation
- `example.before`: SparkSubmitOperator with application, conn_id, conf, deploy_mode, executor_memory (use RESEARCH.md example)
- `example.after`: Show BOTH options — Option 1 with ShellOperation + spark-submit command (stream_output=True), and Option 2 with DatabricksSubmitRun spark_python_task reference (use RESEARCH.md examples)
- `notes`:
  - THREE execution paths — choose based on infrastructure:
    1. Self-managed Spark cluster: Use ShellOperation with spark-submit (requires spark-submit on PATH)
    2. Databricks-managed Spark: Use prefect-databricks DatabricksSubmitRun with spark_python_task
    3. GCP Dataproc: Use prefect-gcp DataprocSubmitJob
  - ShellOperation with stream_output=True preserves Spark logs in Prefect
  - Do NOT use subprocess.run or os.system — ShellOperation integrates with Prefect logging
  - spark_conn_id → for self-managed: configure Spark master URL as environment variable or Secret block
  - For conf dict: pass as spark-submit --conf flags in the command string
- `related_concepts`: ["shell-operation-pattern", "managed-spark-alternative"]
  </action>
  <verify>
    <automated>grep -c "{% section SparkSubmitOperator %}" colin/models/operators/spark.md && grep -q "ShellOperation" colin/models/operators/spark.md && grep -q "stream_output" colin/models/operators/spark.md && echo "PASS" || echo "FAIL"</automated>
  </verify>
  <done>spark.md exists with SparkSubmitOperator section containing three execution paths (shell, Databricks, Dataproc), ShellOperation as primary pattern, and explicit anti-pattern against subprocess.run</done>
</task>

<task type="auto">
  <name>Task 3: Update _index.md to reference all new operator model files</name>
  <files>colin/models/operators/_index.md</files>
  <action>
Edit `colin/models/operators/_index.md` to add five new `{{ ref() }}` sections for the new operator model files. Add them AFTER the existing Database Operators section.

Add these sections in this order:
```markdown
## Kubernetes Provider Operators

{{ ref('operators/kubernetes').content }}

## Databricks Provider Operators

{{ ref('operators/databricks').content }}

## Spark Provider Operators

{{ ref('operators/spark').content }}

## HTTP Provider Operators

{{ ref('operators/http').content }}

## SSH Provider Operators

{{ ref('operators/sftp').content }}
```

This ensures `colin run` includes all new operator files in the compiled output.
  </action>
  <verify>
    <automated>grep -c "ref('operators/" colin/models/operators/_index.md | grep -q "10" && echo "PASS — 10 refs (5 existing + 5 new)" || (echo "Ref count:" && grep -c "ref('operators/" colin/models/operators/_index.md)</automated>
  </verify>
  <done>_index.md references all 10 operator model files (5 existing + 5 new) via {{ ref() }} template includes</done>
</task>

</tasks>

<verification>
All files exist and reference expected operators:
```bash
for f in databricks.md spark.md; do
  echo "=== $f ===" && grep "{% section" colin/models/operators/$f
done
echo "=== _index.md refs ===" && grep "ref('operators/" colin/models/operators/_index.md
```
</verification>

<success_criteria>
1. `colin/models/operators/databricks.md` has both DatabricksSubmitRunOperator and DatabricksRunNowOperator sections with DatabricksCredentials migration
2. `colin/models/operators/spark.md` has SparkSubmitOperator with three execution paths and ShellOperation as primary
3. `colin/models/operators/_index.md` includes {{ ref() }} for kubernetes, databricks, spark, http, and sftp
4. All files follow exact `{% section %}` format with all required fields
</success_criteria>

<output>
After completion, create `.planning/phases/01-p1-knowledge-expansion/01-02-SUMMARY.md`
</output>
