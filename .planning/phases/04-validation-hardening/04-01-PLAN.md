---
phase: 04-validation-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/airflow_unfactor/tools/validate.py
  - tests/fixtures/kubernetes_pod_operator.py
  - tests/fixtures/databricks_submit_run.py
  - tests/fixtures/azure_data_factory.py
  - tests/fixtures/dbt_cloud_run_job.py
  - tests/fixtures/http_operator.py
  - tests/fixtures/ssh_operator.py
  - tests/test_validate.py
autonomous: true
requirements:
  - VALD-01
  - VALD-02

must_haves:
  truths:
    - "validate_conversion on a Kubernetes DAG returns guidance mentioning Kubernetes work pool"
    - "validate_conversion on a Databricks DAG returns guidance mentioning prefect-databricks"
    - "validate_conversion on an Azure DAG returns guidance mentioning prefect-azure"
    - "validate_conversion on a dbt DAG returns guidance mentioning prefect-dbt"
    - "validate_conversion on an HTTP DAG returns guidance mentioning httpx"
    - "validate_conversion on an SSH DAG returns guidance mentioning Secret block"
    - "validate_conversion on a plain DAG (no new operators) returns unchanged base guidance"
    - "All six fixture DAGs exist and are valid Python"
  artifacts:
    - path: "src/airflow_unfactor/tools/validate.py"
      provides: "Conditional operator-specific guidance injection"
      contains: "KubernetesPodOperator"
    - path: "tests/fixtures/kubernetes_pod_operator.py"
      provides: "Kubernetes fixture DAG"
      contains: "KubernetesPodOperator"
    - path: "tests/fixtures/databricks_submit_run.py"
      provides: "Databricks fixture DAG"
      contains: "DatabricksSubmitRunOperator"
    - path: "tests/fixtures/azure_data_factory.py"
      provides: "Azure fixture DAG"
      contains: "AzureDataFactoryRunPipelineOperator"
    - path: "tests/fixtures/dbt_cloud_run_job.py"
      provides: "dbt Cloud fixture DAG"
      contains: "DbtCloudRunJobOperator"
    - path: "tests/fixtures/http_operator.py"
      provides: "HTTP fixture DAG"
      contains: "SimpleHttpOperator"
    - path: "tests/fixtures/ssh_operator.py"
      provides: "SSH fixture DAG"
      contains: "SSHOperator"
    - path: "tests/test_validate.py"
      provides: "TestPhase4Validation integration test class"
      contains: "TestPhase4Validation"
  key_links:
    - from: "src/airflow_unfactor/tools/validate.py"
      to: "tests/test_validate.py"
      via: "validate_conversion called with fixture DAG source"
      pattern: "validate_conversion"
    - from: "tests/fixtures/*.py"
      to: "tests/test_validate.py"
      via: "read_text() loads fixture as original_dag argument"
      pattern: "FIXTURES_DIR.*read_text"
---

<objective>
Add operator-specific checklist items to the validate tool's comparison guidance and verify them against production-style fixture DAGs for all six new operator families.

Purpose: The validate tool currently returns a static 9-item checklist regardless of DAG content. Users converting Kubernetes, Databricks, Azure, dbt, HTTP, or SSH DAGs need targeted guidance specific to those operator types. This plan makes guidance conditional on DAG content and proves it works against realistic fixtures.

Output: Modified validate.py with conditional guidance, six fixture DAGs, and a TestPhase4Validation integration test class.
</objective>

<execution_context>
@/Users/gcoyne/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gcoyne/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-validation-hardening/04-RESEARCH.md
@src/airflow_unfactor/tools/validate.py
@tests/test_validate.py

<interfaces>
<!-- Key function signature the executor needs -->

From src/airflow_unfactor/tools/validate.py:
```python
async def validate_conversion(original_dag: str, converted_flow: str) -> str:
    """Returns JSON with original_source, converted_source, syntax_valid, syntax_errors, comparison_guidance."""
```

From tests/test_validate.py:
```python
# Existing inline content test pattern:
result = json.loads(asyncio.run(validate_conversion(dag_code, flow_code)))
```

From tests/test_knowledge.py (parametrize pattern to replicate):
```python
@pytest.mark.parametrize("operator_name,expected_content", [
    ("AzureDataFactoryRunPipelineOperator", "ARCHITECTURE SHIFT"),
    ...
])
def test_phase3_operator_lookup(self, operator_name, expected_content):
    ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add conditional operator-specific guidance to validate_conversion</name>
  <files>
    src/airflow_unfactor/tools/validate.py
    tests/fixtures/kubernetes_pod_operator.py
    tests/fixtures/databricks_submit_run.py
    tests/fixtures/azure_data_factory.py
    tests/fixtures/dbt_cloud_run_job.py
    tests/fixtures/http_operator.py
    tests/fixtures/ssh_operator.py
  </files>
  <action>
**In `src/airflow_unfactor/tools/validate.py`:**

Refactor `validate_conversion` to build the comparison_guidance as a variable instead of an inline string, then append operator-specific items based on the original DAG source content.

1. Extract the existing 9-item guidance string into a local variable `guidance` before the `return json.dumps(...)` call.

2. After building the base guidance, detect operator families by searching `original_source` (not `original_dag` — use the resolved content) for class name substrings. Append extra lines without numbered prefixes — use descriptive labels instead (e.g., "Kubernetes:", "Databricks:") to avoid numbering conflicts when multiple operator types appear in one DAG.

3. Detection rules (use `in` operator on `original_source`):
   - `"KubernetesPodOperator"` → append: `"Kubernetes: verify a Kubernetes work pool is configured and that image, namespace, and env_vars map to the Prefect infrastructure block"`
   - `any(x in original_source for x in ("DatabricksSubmitRunOperator", "DatabricksRunNowOperator"))` → append: `"Databricks: verify prefect-databricks is installed, DatabricksCredentials block is configured, and job parameters are mapped"`
   - `any(x in original_source for x in ("AzureDataFactoryRunPipelineOperator", "WasbOperator", "WasbDeleteOperator"))` → append: `"Azure: verify prefect-azure is installed and AzureBlobStorageCredentials or AzureDataFactoryCredentials block is configured"`
   - `"DbtCloudRunJobOperator"` → append: `"dbt Cloud: verify prefect-dbt is installed, DbtCloudCredentials block is set, and job ID and account ID are passed"`
   - `any(x in original_source for x in ("SimpleHttpOperator", "HttpOperator"))` → append: `"HTTP: verify httpx is used (no prefect-http package exists), connection ID maps to base_url, and headers/auth are passed explicitly"`
   - `"SSHOperator"` → append: `"SSH: verify paramiko or fabric is used, SSH credentials are in a Prefect Secret block, and host/user/key are not hardcoded"`

4. Join extra items with `"\n"` and append to `guidance` with a preceding `"\n"` separator only if extras exist.

5. Use the `guidance` variable in the returned dict instead of the inline string.

**Important:** Do NOT change the base 9-item guidance text. Existing tests in `TestValidateConversion.test_comparison_guidance_content` assert on that content and must not break.

**Create six fixture DAGs in `tests/fixtures/`:**

Each fixture must be a minimal but realistic production-style Airflow DAG with:
- A `DAG()` context manager with `dag_id`, `start_date`, `schedule_interval`, `catchup=False`
- At least one operator instance with realistic parameters (not just `task_id="x"`)
- A dependency chain of two tasks using `>>`

Fixture files and content guidelines:

- `tests/fixtures/kubernetes_pod_operator.py`: Two `KubernetesPodOperator` tasks with `namespace`, `image`, `env_vars`, `get_logs`. Import from `airflow.providers.cncf.kubernetes.operators.pod`.
- `tests/fixtures/databricks_submit_run.py`: One `DatabricksSubmitRunOperator` task with `json` parameter containing notebook_task config, plus a downstream `PythonOperator`. Import from `airflow.providers.databricks.operators.databricks`.
- `tests/fixtures/azure_data_factory.py`: One `AzureDataFactoryRunPipelineOperator` task with `resource_group_name`, `factory_name`, `pipeline_name`, plus a downstream task. Import from `airflow.providers.microsoft.azure.operators.data_factory`.
- `tests/fixtures/dbt_cloud_run_job.py`: One `DbtCloudRunJobOperator` task with `dbt_cloud_conn_id`, `job_id`, `account_id`, plus a downstream notification task. Import from `airflow.providers.dbt.cloud.operators.dbt`.
- `tests/fixtures/http_operator.py`: One `SimpleHttpOperator` task with `http_conn_id`, `endpoint`, `method`, `headers`, plus a downstream task. Import from `airflow.providers.http.operators.http`.
- `tests/fixtures/ssh_operator.py`: One `SSHOperator` task with `ssh_conn_id`, `command`, plus a downstream task. Import from `airflow.providers.ssh.operators.ssh`.

These are NOT executed — they are source text read by `validate_conversion`. They only need to be syntactically valid Python with realistic Airflow patterns.
  </action>
  <verify>
    <automated>cd /Users/gcoyne/src/prefect/airflow-unfactor && uv run pytest tests/test_validate.py::TestValidateConversion -x -q</automated>
  </verify>
  <done>
    - validate.py builds guidance as a variable and appends operator-specific items when detected
    - Base 9-item guidance unchanged for DAGs without new operators
    - All six fixture DAGs exist and are syntactically valid Python
    - All existing TestValidateConversion tests still pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Add TestPhase4Validation integration tests</name>
  <files>tests/test_validate.py</files>
  <action>
**In `tests/test_validate.py`:**

Add a new `TestPhase4Validation` class after the existing `TestValidateConversion` class.

1. Add imports at the top of the file (if not already present): `from pathlib import Path`, `import pytest`.

2. Define `FIXTURES_DIR = Path(__file__).parent / "fixtures"` at module level (or inside the class).

3. Create the class with a docstring: `"""VALD-01 + VALD-02: operator-specific guidance and production-style fixture DAGs."""`

4. Add a parametrized test method `test_operator_specific_guidance`:

```python
@pytest.mark.parametrize("fixture_name,expected_fragment", [
    ("kubernetes_pod_operator.py", "Kubernetes"),
    ("databricks_submit_run.py", "prefect-databricks"),
    ("azure_data_factory.py", "prefect-azure"),
    ("dbt_cloud_run_job.py", "prefect-dbt"),
    ("http_operator.py", "httpx"),
    ("ssh_operator.py", "Secret block"),
])
def test_operator_specific_guidance(self, fixture_name, expected_fragment):
    """validate_conversion returns operator-specific checklist items."""
    dag_source = (FIXTURES_DIR / fixture_name).read_text()
    flow_stub = "from prefect import flow\n\n@flow\ndef stub(): pass"
    result = json.loads(asyncio.run(validate_conversion(dag_source, flow_stub)))
    guidance = result["comparison_guidance"]
    assert expected_fragment in guidance, (
        f"Expected '{expected_fragment}' in guidance for {fixture_name};\n"
        f"Got: {guidance}"
    )
```

5. Add a second test `test_base_guidance_unchanged_for_plain_dag` that verifies the base 9-item guidance is returned unchanged when the DAG contains no new operators:

```python
def test_base_guidance_unchanged_for_plain_dag(self):
    """Plain DAG without new operators returns only base guidance."""
    dag_source = "from airflow import DAG\nfrom airflow.operators.python import PythonOperator\ndag = DAG('simple')"
    flow_stub = "from prefect import flow\n\n@flow\ndef stub(): pass"
    result = json.loads(asyncio.run(validate_conversion(dag_source, flow_stub)))
    guidance = result["comparison_guidance"]
    assert "Kubernetes" not in guidance
    assert "Databricks" not in guidance
    assert "Azure" not in guidance
    assert "dbt Cloud" not in guidance
    assert "httpx" not in guidance
    assert "Secret block" not in guidance
    # Base items still present
    assert "tasks" in guidance.lower()
    assert "dependencies" in guidance.lower()
```

6. Add a third test `test_multiple_operator_types_in_one_dag` that verifies guidance includes items for multiple operators when both appear in the source:

```python
def test_multiple_operator_types_in_one_dag(self):
    """DAG with multiple new operator types gets all relevant guidance."""
    dag_source = (
        "from airflow import DAG\n"
        "from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n"
        "from airflow.providers.http.operators.http import SimpleHttpOperator\n"
    )
    flow_stub = "from prefect import flow\n\n@flow\ndef stub(): pass"
    result = json.loads(asyncio.run(validate_conversion(dag_source, flow_stub)))
    guidance = result["comparison_guidance"]
    assert "Kubernetes" in guidance
    assert "httpx" in guidance
```
  </action>
  <verify>
    <automated>cd /Users/gcoyne/src/prefect/airflow-unfactor && uv run pytest tests/test_validate.py -x -q</automated>
  </verify>
  <done>
    - TestPhase4Validation class exists with 8 parametrized + 2 standalone tests (10 test cases total)
    - All 6 parametrized fixture tests pass, proving operator-specific guidance works
    - Plain DAG test passes, proving no regression on base guidance
    - Multi-operator test passes, proving guidance stacks correctly
    - Full test suite passes: `uv run pytest -q` (all 136+ existing tests plus new ones)
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_validate.py -x -q` — all tests pass (existing + new)
2. `uv run pytest -q` — full suite green (136+ existing tests plus ~10 new)
3. Manual spot check: the base 9-item guidance string is unchanged in validate.py
4. Each fixture file in tests/fixtures/ is valid Python: `uv run python -m py_compile tests/fixtures/kubernetes_pod_operator.py` (repeat for all six)
</verification>

<success_criteria>
- Calling validate_conversion with a Kubernetes DAG returns guidance containing "Kubernetes work pool" (VALD-01)
- Calling validate_conversion with a Databricks DAG returns guidance containing "prefect-databricks" (VALD-01)
- Six production-style fixture DAGs exist in tests/fixtures/ covering Kubernetes, Databricks, Azure, dbt, HTTP, SSH (VALD-02)
- All existing tests pass without modification (no regression)
- Full test suite green
</success_criteria>

<output>
After completion, create `.planning/phases/04-validation-hardening/04-01-SUMMARY.md`
</output>
