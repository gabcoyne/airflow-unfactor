---
phase: 03-p2-knowledge-expansion
plan: 03
type: execute
wave: 2
depends_on:
  - "03-01"
  - "03-02"
files_modified:
  - colin/output/operators-azure.json
  - colin/output/operators-dbt.json
  - colin/output/patterns.json
  - colin/output/concepts.json
  - tests/test_knowledge.py
autonomous: true
requirements:
  - KNOW-07
  - KNOW-08
  - KNOW-09
  - KNOW-10
  - KNOW-11
  - KNOW-12

must_haves:
  truths:
    - "lookup_concept('AzureDataFactoryRunPipelineOperator') returns found status with ARCHITECTURE SHIFT content"
    - "lookup_concept('WasbOperator') returns found status with prefect-azure blob_storage pattern"
    - "lookup_concept('DbtCloudRunJobOperator') returns found status with prefect-dbt pattern"
    - "lookup_concept('macros.ds_add') returns found status with Python datetime equivalent"
    - "lookup_concept('depends_on_past') returns found status with workaround section"
    - "lookup_concept('deferrable') returns found status with Prefect alternatives"
    - "All tests pass including new parametrized operator and concept tests"
  artifacts:
    - path: "colin/output/operators-azure.json"
      provides: "Compiled Azure operator knowledge"
      contains: "AzureDataFactoryRunPipelineOperator"
    - path: "colin/output/operators-dbt.json"
      provides: "Compiled dbt operator knowledge"
      contains: "DbtCloudRunJobOperator"
    - path: "tests/test_knowledge.py"
      provides: "Parametrized tests for all Phase 3 operators and concepts"
      contains: "AzureDataFactoryRunPipelineOperator"
  key_links:
    - from: "tests/test_knowledge.py"
      to: "colin/output/*.json"
      via: "load_knowledge() reads compiled JSON"
      pattern: "load_knowledge"
    - from: "src/airflow_unfactor/knowledge.py"
      to: "colin/output/*.json"
      via: "load_knowledge glob for *.json files"
      pattern: "glob.*json"
---

<objective>
Compile Colin models to JSON output and add parametrized integration tests verifying that all Phase 3 operators and concepts are findable via lookup_concept.

Purpose: Ensure the Colin models authored in Plans 01 and 02 are compiled to JSON and actually discoverable through the lookup pipeline. Tests serve as regression guards.
Output: Compiled JSON in colin/output/, parametrized tests covering all Phase 3 requirements.
</objective>

<execution_context>
@/Users/gcoyne/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gcoyne/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-p2-knowledge-expansion/03-01-SUMMARY.md
@.planning/phases/03-p2-knowledge-expansion/03-02-SUMMARY.md

@src/airflow_unfactor/knowledge.py (load_knowledge and lookup functions)
@tests/test_knowledge.py (existing test patterns — extend, don't rewrite)
</context>

<interfaces>
<!-- From knowledge.py -->
```python
def load_knowledge(colin_output_dir: str = "colin/output") -> dict[str, Any]:
    # Reads all *.json from colin_output_dir
    # Returns dict mapping concept names to entries

def lookup(concept: str, knowledge: dict[str, Any]) -> dict[str, Any]:
    # Returns {"status": "found", ...} or {"status": "not_found", ...}
```

<!-- Colin compilation: the project uses `colin run` to compile .md → .json -->
<!-- If colin CLI is unavailable, manually create JSON matching the format load_knowledge expects -->
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Compile Colin models to JSON output</name>
  <files>colin/output/operators-azure.json, colin/output/operators-dbt.json, colin/output/patterns.json, colin/output/concepts.json</files>
  <action>
Attempt to compile Colin models using the project's build process:
```bash
cd /Users/gcoyne/src/prefect/airflow-unfactor && uv run colin run 2>&1 || echo "Colin CLI unavailable"
```

If `colin run` succeeds, verify the output JSON files exist and contain the expected entries.

If `colin run` is unavailable or fails, manually create/update the JSON files to match the format that `load_knowledge()` expects. The loader accepts two formats:
1. `{"entries": [{"name": "X", ...}, ...]}` — list of named entries
2. `{"KeyName": {...}, ...}` — flat dict of concepts

For each new operator model, create JSON in the flat dict format:

**colin/output/operators-azure.json** (create or update):
```json
{
  "AzureDataFactoryRunPipelineOperator": {
    "name": "AzureDataFactoryRunPipelineOperator",
    "concept_type": "operator",
    "intent": "Triggers an Azure Data Factory pipeline and polls for completion...",
    "airflow": {"name": "AzureDataFactoryRunPipelineOperator", "module": "airflow.providers.microsoft.azure.operators.data_factory"},
    "prefect_equivalent": {
      "pattern": "ARCHITECTURE SHIFT: azure-mgmt-datafactory SDK in @task",
      "package": "none (use azure-mgmt-datafactory + azure-identity)",
      "import": "from azure.mgmt.datafactory import DataFactoryManagementClient"
    },
    "translation_rules": [...]
  },
  "WasbOperator": { ... },
  "WasbDeleteOperator": { ... }
}
```

**colin/output/operators-dbt.json** (create or update):
```json
{
  "DbtCloudRunJobOperator": {
    "name": "DbtCloudRunJobOperator",
    "concept_type": "operator",
    "intent": "Triggers a dbt Cloud job and waits for completion...",
    "airflow": {"name": "DbtCloudRunJobOperator", "module": "airflow.providers.dbt.cloud.operators.dbt"},
    "prefect_equivalent": {
      "pattern": "trigger_dbt_cloud_job_run_and_wait_for_completion",
      "package": "prefect-dbt[cloud]",
      "import": "from prefect_dbt.cloud.jobs import trigger_dbt_cloud_job_run_and_wait_for_completion"
    },
    "translation_rules": [...]
  }
}
```

For patterns.md and concepts.md updates: update existing `colin/output/patterns.json` and `colin/output/concepts.json` to include the new entries (jinja-template-variables, depends-on-past, deferrable-operators). Read the existing files first and merge new entries.

The JSON must be valid and parseable by `load_knowledge()`. Verify by running:
```bash
uv run python -c "from airflow_unfactor.knowledge import load_knowledge; k = load_knowledge(); print([x for x in k.keys() if 'Azure' in x or 'Dbt' in x or 'Wasb' in x or 'depends' in x or 'deferrable' in x or 'jinja' in x])"
```

Force-add output JSON to git (colin/output/ may be in .gitignore — Phase 1 precedent: force-add):
```bash
git add -f colin/output/operators-azure.json colin/output/operators-dbt.json colin/output/patterns.json colin/output/concepts.json
```
  </action>
  <verify>
    <automated>cd /Users/gcoyne/src/prefect/airflow-unfactor && uv run python -c "from airflow_unfactor.knowledge import load_knowledge, lookup; k = load_knowledge(); r = lookup('AzureDataFactoryRunPipelineOperator', k); print(r['status'])" 2>&1 | grep -q "found" && echo "PASS" || echo "FAIL"</automated>
  </verify>
  <done>Colin output JSON files exist with all Phase 3 operators and concepts; load_knowledge() successfully loads them; lookup() returns found for AzureDataFactoryRunPipelineOperator, WasbOperator, DbtCloudRunJobOperator</done>
</task>

<task type="auto">
  <name>Task 2: Add parametrized integration tests for all Phase 3 entries</name>
  <files>tests/test_knowledge.py</files>
  <action>
Add parametrized tests to `tests/test_knowledge.py` verifying all Phase 3 requirements end-to-end (Colin JSON → load_knowledge → lookup → found).

Use the real `colin/output/` directory (not mocks) for integration testing — same pattern as Phase 1 tests.

**Test 1: Parametrized operator lookup tests**

```python
@pytest.mark.parametrize("operator_name,expected_content", [
    ("AzureDataFactoryRunPipelineOperator", "ARCHITECTURE SHIFT"),
    ("WasbOperator", "AzureBlobStorageCredentials"),
    ("DbtCloudRunJobOperator", "trigger_dbt_cloud_job_run_and_wait_for_completion"),
])
def test_phase3_operator_lookup(self, operator_name, expected_content):
    """Phase 3 operators are findable via lookup."""
    knowledge = load_knowledge()
    result = lookup(operator_name, knowledge)
    assert result["status"] == "found"
    # Verify the result contains expected content somewhere in its values
    result_str = json.dumps(result)
    assert expected_content in result_str
```

**Test 2: Concept lookup tests**

```python
@pytest.mark.parametrize("concept,expected_in_result", [
    ("depends_on_past", "no direct equivalent"),
    ("deferrable", "Automations"),
])
def test_phase3_concept_lookup(self, concept, expected_in_result):
    """Phase 3 concepts are findable via lookup."""
    knowledge = load_knowledge()
    result = lookup(concept, knowledge)
    assert result["status"] == "found"
    result_str = json.dumps(result).lower()
    assert expected_in_result.lower() in result_str
```

**Test 3: Jinja macro lookup with normalization**

```python
@pytest.mark.parametrize("query", [
    "macros.ds_add",
    "{{ macros.ds_add(ds, 5) }}",
    "dag_run.conf",
    "{{ dag_run.conf }}",
    "var.value.my_key",
])
def test_phase3_jinja_macro_lookup(self, query):
    """Jinja macro queries are findable after normalization."""
    knowledge = load_knowledge()
    result = lookup(query, knowledge)
    assert result["status"] == "found", f"'{query}' returned not_found"
```

**Test 4: Schedule scaffold integration** (in test_scaffold.py, already covered by Plan 02 — skip if already present)

Place these tests in a new class `TestPhase3Integration` in test_knowledge.py to keep them grouped.

Ensure all tests pass:
```bash
uv run pytest tests/test_knowledge.py -x -q
```
  </action>
  <verify>
    <automated>cd /Users/gcoyne/src/prefect/airflow-unfactor && uv run pytest tests/test_knowledge.py tests/test_scaffold.py -x -q 2>&1 | tail -5</automated>
  </verify>
  <done>All parametrized tests pass; every Phase 3 operator and concept is verifiable end-to-end through the lookup pipeline; full test suite passes with no regressions</done>
</task>

</tasks>

<verification>
- `uv run pytest -x -q` — full test suite passes
- Every requirement ID has at least one passing test:
  - KNOW-07: AzureDataFactoryRunPipelineOperator + WasbOperator lookups pass
  - KNOW-08: DbtCloudRunJobOperator lookup passes
  - KNOW-09: Jinja macro lookups (macros.ds_add, dag_run.conf, var.value.x) pass
  - KNOW-10: depends_on_past lookup returns "no direct equivalent" with workaround
  - KNOW-11: deferrable operator lookup returns Automations guidance
  - KNOW-12: scaffold schedule tests pass (cron, interval, none, @once)
</verification>

<success_criteria>
1. Colin output JSON files contain all Phase 3 operators and concepts
2. `load_knowledge()` successfully loads all new entries
3. Parametrized tests verify end-to-end lookup for every Phase 3 requirement
4. Full test suite passes: `uv run pytest -x -q`
5. No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/03-p2-knowledge-expansion/03-03-SUMMARY.md`
</output>
