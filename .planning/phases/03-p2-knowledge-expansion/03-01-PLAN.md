---
phase: 03-p2-knowledge-expansion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - colin/models/operators/azure.md
  - colin/models/operators/dbt.md
  - colin/models/operators/_index.md
autonomous: true
requirements:
  - KNOW-07
  - KNOW-08

must_haves:
  truths:
    - "lookup_concept('AzureDataFactoryRunPipelineOperator') returns an ARCHITECTURE SHIFT entry with azure-mgmt-datafactory SDK pattern, not not_found"
    - "lookup_concept('WasbOperator') returns prefect-azure AzureBlobStorageCredentials + blob_storage task mapping"
    - "lookup_concept('DbtCloudRunJobOperator') returns prefect-dbt trigger_dbt_cloud_job_run_and_wait_for_completion pattern"
    - "Each Colin model has an ## intent section before ## prefect_pattern explaining what the operator achieves"
  artifacts:
    - path: "colin/models/operators/azure.md"
      provides: "Azure operator Colin models (AzureDataFactoryRunPipelineOperator, WasbOperator, WasbDeleteOperator)"
      contains: "{% section AzureDataFactoryRunPipelineOperator %}"
    - path: "colin/models/operators/dbt.md"
      provides: "dbt Cloud operator Colin model (DbtCloudRunJobOperator)"
      contains: "{% section DbtCloudRunJobOperator %}"
    - path: "colin/models/operators/_index.md"
      provides: "Updated index referencing azure.md and dbt.md"
      contains: "operators/azure"
  key_links:
    - from: "colin/models/operators/_index.md"
      to: "colin/models/operators/azure.md"
      via: "ref('operators/azure')"
      pattern: "ref\\('operators/azure'\\)"
    - from: "colin/models/operators/_index.md"
      to: "colin/models/operators/dbt.md"
      via: "ref('operators/dbt')"
      pattern: "ref\\('operators/dbt'\\)"
---

<objective>
Author Colin models for Azure operators (AzureDataFactoryRunPipelineOperator, WasbOperator) and dbt Cloud operator (DbtCloudRunJobOperator), following the intent-first approach established in Phase 3 CONTEXT.md.

Purpose: Users migrating Azure Data Factory, Azure Blob Storage, and dbt Cloud DAGs get authoritative translation guidance with explicit "what this achieves" framing.
Output: Two new Colin model files (azure.md, dbt.md) registered in _index.md.
</objective>

<execution_context>
@/Users/gcoyne/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gcoyne/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-p2-knowledge-expansion/03-RESEARCH.md

@colin/models/operators/kubernetes.md (gold standard for intent-first approach — use this as the template)
@colin/models/operators/_index.md (must be updated to reference new files)
</context>

<interfaces>
<!-- Colin model {% section %} template from RESEARCH.md -->
Each operator section must follow this structure:
```
{% section OperatorName %}
## operator
## module
## intent       ← NEW in Phase 3: "what this achieves" (2-3 sentences, user perspective)
## source_context
## prefect_pattern
## prefect_package
## prefect_import
## example
### before / ### after
## notes
## related_concepts
{% endsection %}
```

<!-- _index.md pattern from existing file -->
_index.md uses `{{ ref('operators/<name>').content }}` to aggregate.
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Author Azure Colin models (azure.md)</name>
  <files>colin/models/operators/azure.md</files>
  <action>
Create `colin/models/operators/azure.md` with three sections, following the kubernetes.md intent-first template:

**Section 1: AzureDataFactoryRunPipelineOperator** (ARCHITECTURE SHIFT entry)
- `## intent`: "Triggers an Azure Data Factory pipeline and polls for completion. Used when DAGs orchestrate ADF pipelines as part of data processing workflows."
- `## source_context`: Operator uses `azure_data_factory_conn_id`, `pipeline_name`, `resource_group_name`, `factory_name`, `parameters`, `wait_for_termination`.
- `## prefect_pattern`: ARCHITECTURE SHIFT — prefect-azure has no ADF module. Use `azure-mgmt-datafactory` SDK directly inside a `@task`. Create `ClientSecretCredential` from Prefect Secret blocks, instantiate `DataFactoryManagementClient`, call `pipelines.create_run()`.
- `## prefect_package`: none (use azure-mgmt-datafactory + azure-identity directly)
- `## prefect_import`: `from azure.identity import ClientSecretCredential` and `from azure.mgmt.datafactory import DataFactoryManagementClient` and `from prefect.blocks.system import Secret`
- `## example`: Show before (AzureDataFactoryRunPipelineOperator) and after (@task with SDK) from RESEARCH.md Pattern 2.
- `## notes`: MUST include warning "No prefect-azure ADF module exists — do NOT attempt `from prefect_azure import DataFactory*`". Include polling pattern with `get_pipeline_run()`. Mention `@task(retries=3, retry_delay_seconds=30)` for resilience. Do NOT use AzureContainerInstanceCredentials for ADF.

**Section 2: WasbOperator**
- `## intent`: "Reads or writes blob data to Azure Blob Storage using the wasb:// protocol. Used in DAGs that process files stored in Azure containers."
- `## prefect_pattern`: `AzureBlobStorageCredentials` block + `blob_storage_download`/`blob_storage_upload` tasks from `prefect_azure.blob_storage`.
- `## prefect_package`: prefect-azure[blob_storage]
- `## example`: Show before (WasbOperator) and after (AzureBlobStorageCredentials + blob_storage_download) from RESEARCH.md Pattern 1. Include credential block setup.
- `## notes`: Use `AzureBlobStorageCredentials` NOT `AzureBlobCredentials` (the latter is for flow code storage). Show both connection_string and service_principal auth methods.

**Section 3: WasbDeleteOperator** (per RESEARCH.md Open Question 3 recommendation)
- `## intent`: "Deletes blobs from Azure Blob Storage. Companion to WasbOperator for cleanup tasks."
- Same credential pattern as WasbOperator. Use `blob_storage_list` + delete approach or direct Azure SDK if needed.
- Brief entry — point to WasbOperator section for credential setup details.

All sections must have the `## intent` section BEFORE `## prefect_pattern` per CONTEXT.md locked decision.
  </action>
  <verify>
    <automated>test -f colin/models/operators/azure.md && grep -c "{% section" colin/models/operators/azure.md | grep -q "3" && grep -q "## intent" colin/models/operators/azure.md && grep -q "ARCHITECTURE SHIFT" colin/models/operators/azure.md && echo "PASS" || echo "FAIL"</automated>
  </verify>
  <done>azure.md exists with 3 sections (AzureDataFactoryRunPipelineOperator, WasbOperator, WasbDeleteOperator), each containing ## intent before ## prefect_pattern, and ADF entry contains ARCHITECTURE SHIFT warning</done>
</task>

<task type="auto">
  <name>Task 2: Author dbt Cloud Colin model (dbt.md) and update _index.md</name>
  <files>colin/models/operators/dbt.md, colin/models/operators/_index.md</files>
  <action>
**Part A: Create `colin/models/operators/dbt.md`** with one section:

**Section: DbtCloudRunJobOperator**
- `## intent`: "Triggers a dbt Cloud job and waits for it to complete. Used in DAGs that orchestrate dbt transformations as part of an ELT pipeline."
- `## source_context`: Operator uses `dbt_cloud_conn_id`, `job_id`, `account_id`, `trigger_reason`, `wait_for_termination` (True by default).
- `## prefect_pattern`: `trigger_dbt_cloud_job_run_and_wait_for_completion` from `prefect_dbt.cloud.jobs` with `DbtCloudCredentials` block.
- `## prefect_package`: prefect-dbt[cloud]
- `## prefect_import`: `from prefect_dbt.cloud import DbtCloudCredentials` and `from prefect_dbt.cloud.jobs import trigger_dbt_cloud_job_run_and_wait_for_completion`
- `## example`: Show before (DbtCloudRunJobOperator) and after (trigger_dbt_cloud_job_run_and_wait_for_completion) from RESEARCH.md Pattern 3. Include DbtCloudCredentials block setup.
- `## notes`: Use `trigger_dbt_cloud_job_run_and_wait_for_completion` NOT the fire-and-forget `trigger_dbt_cloud_job_run` — DbtCloudRunJobOperator waits by default. Mention `retry_filtered_models_attempts` parameter. Include credential block creation snippet.

Must have `## intent` before `## prefect_pattern` per CONTEXT.md locked decision.

**Part B: Update `colin/models/operators/_index.md`** to include azure.md and dbt.md:

Add two new sections at the end (before closing):
```
## Azure Provider Operators

{{ ref('operators/azure').content }}

## dbt Provider Operators

{{ ref('operators/dbt').content }}
```
  </action>
  <verify>
    <automated>test -f colin/models/operators/dbt.md && grep -q "{% section DbtCloudRunJobOperator %}" colin/models/operators/dbt.md && grep -q "## intent" colin/models/operators/dbt.md && grep -q "operators/azure" colin/models/operators/_index.md && grep -q "operators/dbt" colin/models/operators/_index.md && echo "PASS" || echo "FAIL"</automated>
  </verify>
  <done>dbt.md exists with DbtCloudRunJobOperator section containing ## intent, and _index.md references both azure.md and dbt.md</done>
</task>

</tasks>

<verification>
- All three new operator models (AzureDataFactoryRunPipelineOperator, WasbOperator, DbtCloudRunJobOperator) use the intent-first template with `## intent` before `## prefect_pattern`
- ADF entry explicitly warns that no prefect-azure ADF module exists
- WasbOperator uses `AzureBlobStorageCredentials` (not `AzureBlobCredentials`)
- dbt entry uses `trigger_dbt_cloud_job_run_and_wait_for_completion` (not fire-and-forget variant)
- _index.md aggregates both new files
</verification>

<success_criteria>
1. `colin/models/operators/azure.md` contains 3 {% section %} blocks with intent-first structure
2. `colin/models/operators/dbt.md` contains 1 {% section %} block with intent-first structure
3. `colin/models/operators/_index.md` references both new files via `ref()` syntax
4. No references to nonexistent `prefect_azure.DataFactory*` modules
5. All before/after examples are syntactically valid Python
</success_criteria>

<output>
After completion, create `.planning/phases/03-p2-knowledge-expansion/03-01-SUMMARY.md`
</output>
