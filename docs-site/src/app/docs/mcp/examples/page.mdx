# MCP Examples

Practical examples of using the MCP tools for Airflow→Prefect migration.

## Basic Workflow

### 1. Analyze a DAG

```json
{
  "tool": "analyze",
  "args": {
    "path": "dags/etl_pipeline.py"
  }
}
```

**Response:**
```json
{
  "dag_id": "etl_pipeline",
  "airflow_version": "2.x",
  "structure": {
    "operators": [
      {"task_id": "extract", "type": "PythonOperator", "line": 15},
      {"task_id": "transform", "type": "PythonOperator", "line": 22},
      {"task_id": "load", "type": "S3CreateObjectOperator", "line": 29}
    ],
    "dependencies": [
      ["extract", "transform"],
      ["transform", "load"]
    ]
  },
  "patterns": {
    "xcom": [
      {"task_id": "transform", "pattern": "ti.xcom_pull(task_ids='extract')"}
    ],
    "connections": [
      {"connection_id": "aws_default", "used_by": ["load"]}
    ]
  },
  "migration_notes": [
    "Convert XCom to return values",
    "Replace aws_default with S3 credentials block"
  ],
  "conversion_runbook_md": "# Migration Runbook: etl_pipeline\n\n..."
}
```

### 2. Get Prefect Context

```json
{
  "tool": "get_context",
  "args": {
    "topics": ["flows", "tasks", "blocks"],
    "detected_features": ["xcom", "connections"]
  }
}
```

**Response:**
```json
{
  "documentation": {
    "flows": "Flows are the core unit of work in Prefect...",
    "tasks": "Tasks are functions decorated with @task...",
    "blocks": "Blocks store configuration and secrets..."
  },
  "operator_patterns": {
    "PythonOperator": {
      "prefect_pattern": "@task decorator",
      "example": "@task\ndef my_task():\n    ..."
    },
    "S3CreateObjectOperator": {
      "prefect_pattern": "s3_upload from prefect-aws",
      "package": "prefect-aws",
      "example": "from prefect_aws import s3_upload\n..."
    }
  },
  "connection_mappings": {
    "aws_default": {
      "block_type": "AwsCredentials",
      "package": "prefect-aws"
    }
  }
}
```

### 3. LLM Generates Code

Using the analysis and context, the LLM generates idiomatic Prefect code:

```python
from prefect import flow, task
from prefect_aws import AwsCredentials
from prefect_aws.s3 import s3_upload

@task
def extract() -> dict:
    """Extract data from source."""
    return {"data": [...]}

@task
def transform(raw_data: dict) -> dict:
    """Transform extracted data."""
    # Receives data as parameter (replaces XCom)
    return {"transformed": process(raw_data)}

@task
def load(data: dict) -> None:
    """Load data to S3."""
    aws_creds = AwsCredentials.load("aws-prod")
    s3_upload(
        data=data,
        bucket="my-bucket",
        key="output/data.json",
        aws_credentials=aws_creds
    )

@flow(name="etl_pipeline")
def etl_pipeline():
    raw = extract()
    transformed = transform(raw)
    load(transformed)
```

### 4. Validate the Conversion

```json
{
  "tool": "validate",
  "args": {
    "original_dag": "dags/etl_pipeline.py",
    "converted_flow": "flows/etl_pipeline.py"
  }
}
```

**Response:**
```json
{
  "is_valid": true,
  "task_count_match": true,
  "dependency_preserved": true,
  "confidence_score": 92,
  "issues": [],
  "dag_tasks": ["extract", "transform", "load"],
  "flow_tasks": ["extract", "transform", "load"]
}
```

## Complex Patterns

### TaskFlow API DAG

**Input:**
```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(schedule="@daily", start_date=datetime(2024, 1, 1))
def taskflow_etl():
    @task
    def extract():
        return {"data": [1, 2, 3]}

    @task
    def transform(data):
        return [x * 2 for x in data["data"]]

    @task
    def load(results):
        print(f"Loading {len(results)} items")

    data = extract()
    transformed = transform(data)
    load(transformed)

taskflow_etl()
```

**Analysis highlights:**
```json
{
  "patterns": {
    "taskflow": {
      "detected": true,
      "decorators": ["@dag", "@task"],
      "note": "Maps directly to Prefect @flow/@task"
    }
  },
  "migration_notes": [
    "TaskFlow DAGs map cleanly to Prefect",
    "@dag → @flow, @task → @task",
    "Return values already replace XCom"
  ]
}
```

### DAG with Sensors

**Input:**
```python
from airflow import DAG
from airflow.sensors.s3_key_sensor import S3KeySensor
from airflow.operators.python import PythonOperator

with DAG("sensor_example") as dag:
    wait_for_file = S3KeySensor(
        task_id="wait_for_file",
        bucket_name="my-bucket",
        bucket_key="input/*.csv",
        aws_conn_id="aws_default"
    )

    process = PythonOperator(
        task_id="process",
        python_callable=process_file
    )

    wait_for_file >> process
```

**Analysis highlights:**
```json
{
  "patterns": {
    "sensors": [
      {
        "task_id": "wait_for_file",
        "type": "S3KeySensor",
        "suggestions": [
          "Convert to polling task with retries",
          "Or use S3 event-driven trigger"
        ]
      }
    ]
  }
}
```

**Generated Prefect code:**
```python
from prefect import flow, task
from prefect_aws import AwsCredentials
import boto3
import time

@task(retries=60, retry_delay_seconds=60)
def wait_for_s3_file(bucket: str, prefix: str) -> str:
    """Poll S3 for file existence."""
    aws_creds = AwsCredentials.load("aws-prod")
    s3 = boto3.client("s3", **aws_creds.get_boto3_session().get_credentials())

    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
    if response.get("Contents"):
        return response["Contents"][0]["Key"]

    raise Exception("File not found, will retry")

@task
def process_file(file_key: str):
    """Process the detected file."""
    ...

@flow(name="sensor_example")
def sensor_example():
    file_key = wait_for_s3_file("my-bucket", "input/")
    process_file(file_key)
```

### DAG with Dynamic Tasks

**Input:**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator

with DAG("dynamic_tasks") as dag:
    for region in ["us-east-1", "eu-west-1", "ap-south-1"]:
        PythonOperator(
            task_id=f"process_{region}",
            python_callable=process_region,
            op_kwargs={"region": region}
        )
```

**Analysis highlights:**
```json
{
  "patterns": {
    "dynamic_mapping": {
      "detected": true,
      "type": "loop_generation",
      "items": ["us-east-1", "eu-west-1", "ap-south-1"],
      "note": "Convert to .map() for parallel execution"
    }
  }
}
```

**Generated Prefect code:**
```python
from prefect import flow, task

@task
def process_region(region: str):
    """Process data for a specific region."""
    ...

@flow(name="dynamic_tasks")
def dynamic_tasks():
    regions = ["us-east-1", "eu-west-1", "ap-south-1"]
    # Use .map() for parallel execution
    process_region.map(regions)
```

## Operator-Specific Lookups

### Get Specific Operator Mapping

```json
{
  "tool": "operator_mapping",
  "args": {
    "operator_type": "BigQueryInsertJobOperator"
  }
}
```

**Response:**
```json
{
  "airflow_operator": "BigQueryInsertJobOperator",
  "prefect_function": "bigquery_query",
  "package": "prefect-gcp",
  "example": "from prefect_gcp.bigquery import bigquery_query\n\n@task\ndef run_bq_job():\n    return bigquery_query(\n        query=\"SELECT * FROM dataset.table\",\n        gcp_credentials=GcpCredentials.load(\"gcp-prod\")\n    )",
  "migration_notes": [
    "Install prefect-gcp: pip install prefect-gcp",
    "Create GcpCredentials block with service account",
    "Query results returned directly (no XCom needed)"
  ]
}
```

### Get Connection Mapping

```json
{
  "tool": "connection_mapping",
  "args": {
    "connection_id": "postgres_default"
  }
}
```

**Response:**
```json
{
  "airflow_connection": "postgres_default",
  "prefect_block": "SqlAlchemyConnector",
  "package": "prefect-sqlalchemy",
  "example": "from prefect_sqlalchemy import SqlAlchemyConnector\n\nconnector = SqlAlchemyConnector.load(\"postgres-prod\")\nwith connector.get_connection() as conn:\n    result = conn.execute(query)",
  "setup_instructions": [
    "Install prefect-sqlalchemy: pip install prefect-sqlalchemy",
    "Create block via UI or code with connection details",
    "Load block in flow using SqlAlchemyConnector.load()"
  ]
}
```

## Prompt Templates

### Claude Code / Cursor

> Analyze the DAG at `dags/my_etl.py` using the analyze tool, get Prefect context for the detected features, then generate a complete Prefect flow following prefecthq/flows conventions. Finally, validate the conversion.

### Agentic Workflow

```
1. Call analyze(path="dags/my_etl.py")
2. Extract detected_features from patterns
3. Call get_context(topics=["flows", "tasks", "blocks"], detected_features=features)
4. Generate Prefect flow using analysis + context
5. Call validate(original_dag="dags/my_etl.py", converted_flow="flows/my_etl.py")
6. If validation fails, review issues and regenerate
```
