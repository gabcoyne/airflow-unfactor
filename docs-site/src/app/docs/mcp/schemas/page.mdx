# Schemas & Validation

Every MCP tool includes a JSON Schema contract for deterministic tool usage.

## Why JSON Schemas?

Schemas provide:

- **Validation**: Catch malformed input before processing
- **Documentation**: The schema is the authoritative contract
- **Tooling**: IDE support, type generation, auto-completion
- **Determinism**: Agents know exactly what to send and expect

## Tool Schemas

### analyze

Comprehensive DAG analysis returning everything an LLM needs for conversion.

**Input Schema:**
```json
{
  "type": "object",
  "properties": {
    "path": {
      "type": "string",
      "description": "Path to the Airflow DAG file"
    },
    "content": {
      "type": "string",
      "description": "Raw DAG code (alternative to path)"
    },
    "include_external_context": {
      "type": "boolean",
      "default": true,
      "description": "Include external MCP context in response"
    }
  },
  "oneOf": [
    { "required": ["path"] },
    { "required": ["content"] }
  ]
}
```

**Output Schema:**
```json
{
  "type": "object",
  "properties": {
    "dag_id": { "type": "string" },
    "airflow_version": { "type": "string", "enum": ["1.x", "2.x", "3.x"] },
    "structure": {
      "type": "object",
      "properties": {
        "operators": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "task_id": { "type": "string" },
              "type": { "type": "string" },
              "line": { "type": "integer" }
            }
          }
        },
        "dependencies": {
          "type": "array",
          "items": {
            "type": "array",
            "items": { "type": "string" },
            "minItems": 2,
            "maxItems": 2
          }
        },
        "task_groups": { "type": "array" },
        "imports": { "type": "array", "items": { "type": "string" } }
      }
    },
    "patterns": {
      "type": "object",
      "properties": {
        "xcom": { "type": "array" },
        "sensors": { "type": "array" },
        "trigger_rules": { "type": "array" },
        "connections": { "type": "array" },
        "variables": { "type": "array" },
        "taskflow": { "type": "object" },
        "dynamic_mapping": { "type": "object" }
      }
    },
    "dag_config": {
      "type": "object",
      "properties": {
        "schedule": { "type": "string" },
        "catchup": { "type": "boolean" },
        "default_args": { "type": "object" },
        "tags": { "type": "array", "items": { "type": "string" } }
      }
    },
    "complexity": {
      "type": "object",
      "properties": {
        "score": { "type": "integer", "minimum": 0, "maximum": 100 },
        "factors": { "type": "array", "items": { "type": "string" } }
      }
    },
    "migration_notes": { "type": "array", "items": { "type": "string" } },
    "conversion_runbook_md": { "type": "string" },
    "original_code": { "type": "string" }
  },
  "required": ["dag_id", "structure", "patterns"]
}
```

### get_context

Fetch Prefect documentation and patterns for detected features.

**Input Schema:**
```json
{
  "type": "object",
  "properties": {
    "topics": {
      "type": "array",
      "items": {
        "type": "string",
        "enum": ["flows", "tasks", "blocks", "deployments", "work-pools", "automations"]
      },
      "description": "Prefect topics to fetch documentation for"
    },
    "detected_features": {
      "type": "array",
      "items": { "type": "string" },
      "description": "Features detected in DAG analysis"
    }
  },
  "required": ["topics"]
}
```

**Output Schema:**
```json
{
  "type": "object",
  "properties": {
    "documentation": {
      "type": "object",
      "additionalProperties": { "type": "string" }
    },
    "operator_patterns": {
      "type": "object",
      "additionalProperties": {
        "type": "object",
        "properties": {
          "prefect_pattern": { "type": "string" },
          "package": { "type": "string" },
          "example": { "type": "string" }
        }
      }
    },
    "connection_mappings": {
      "type": "object",
      "additionalProperties": {
        "type": "object",
        "properties": {
          "block_type": { "type": "string" },
          "package": { "type": "string" }
        }
      }
    },
    "deployment_template": { "type": "string" }
  }
}
```

### operator_mapping

Get Prefect equivalent for a specific Airflow operator.

**Input Schema:**
```json
{
  "type": "object",
  "properties": {
    "operator_type": {
      "type": "string",
      "description": "Airflow operator class name"
    }
  },
  "required": ["operator_type"]
}
```

**Output Schema:**
```json
{
  "type": "object",
  "properties": {
    "airflow_operator": { "type": "string" },
    "prefect_function": { "type": "string" },
    "package": { "type": "string" },
    "example": { "type": "string" },
    "migration_notes": {
      "type": "array",
      "items": { "type": "string" }
    }
  }
}
```

### connection_mapping

Map Airflow connection to Prefect block type.

**Input Schema:**
```json
{
  "type": "object",
  "properties": {
    "connection_id": {
      "type": "string",
      "description": "Airflow connection ID"
    }
  },
  "required": ["connection_id"]
}
```

**Output Schema:**
```json
{
  "type": "object",
  "properties": {
    "airflow_connection": { "type": "string" },
    "prefect_block": { "type": "string" },
    "package": { "type": "string" },
    "example": { "type": "string" },
    "setup_instructions": {
      "type": "array",
      "items": { "type": "string" }
    }
  }
}
```

### validate

Verify generated Prefect code matches original DAG structure.

**Input Schema:**
```json
{
  "type": "object",
  "properties": {
    "original_dag": {
      "type": "string",
      "description": "Path to original Airflow DAG or raw code"
    },
    "converted_flow": {
      "type": "string",
      "description": "Path to converted Prefect flow or raw code"
    }
  },
  "required": ["original_dag", "converted_flow"]
}
```

**Output Schema:**
```json
{
  "type": "object",
  "properties": {
    "is_valid": { "type": "boolean" },
    "task_count_match": { "type": "boolean" },
    "dependency_preserved": { "type": "boolean" },
    "confidence_score": {
      "type": "integer",
      "minimum": 0,
      "maximum": 100
    },
    "issues": {
      "type": "array",
      "items": { "type": "string" }
    },
    "dag_tasks": {
      "type": "array",
      "items": { "type": "string" }
    },
    "flow_tasks": {
      "type": "array",
      "items": { "type": "string" }
    },
    "dag_edges": {
      "type": "array",
      "items": {
        "type": "array",
        "items": { "type": "string" }
      }
    },
    "flow_edges": {
      "type": "array",
      "items": {
        "type": "array",
        "items": { "type": "string" }
      }
    }
  },
  "required": ["is_valid", "confidence_score"]
}
```

## Validation Philosophy

### Fail Fast

Invalid input is rejected immediately with clear error messages:

```json
{
  "status": "error",
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Missing required field: path or content",
    "schema_path": "$.oneOf"
  }
}
```

### Return Warnings

Partial success includes warnings for review:

```json
{
  "status": "success",
  "data": { ... },
  "warnings": [
    "Unknown operator type: MyCustomOperator",
    "Dynamic task generation detected - review carefully"
  ]
}
```

### Include Context

Errors include enough context to diagnose:

```json
{
  "status": "error",
  "error": {
    "code": "PARSE_ERROR",
    "message": "Syntax error in DAG file",
    "line": 42,
    "column": 15,
    "snippet": "def my_function(\n    missing_paren",
    "suggestion": "Check for missing closing parenthesis"
  }
}
```

## Using Schemas

### TypeScript Type Generation

```bash
# Generate types from schemas
npx json-schema-to-typescript schemas/*.json -o types/
```

### Python Validation

```python
from jsonschema import validate
import json

# Load schema
with open("schemas/analyze_input.json") as f:
    schema = json.load(f)

# Validate input
input_data = {"path": "dags/my_etl.py"}
validate(instance=input_data, schema=schema)
```

### Runtime Validation

The MCP server validates all inputs against schemas before processing:

```python
@tool
async def analyze(args: dict) -> dict:
    # Schema validation happens automatically
    # Invalid input raises ValidationError
    ...
```
